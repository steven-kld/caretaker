import os
import json
from zipfile import ZipFile
from lxml import etree
from docx import Document
from pathlib import Path
import openai

# --- CONFIG ---
OPENAI_API_KEY="sk-proj-..."

RAW_DIR = Path("raw")
OUT_DIR = Path("instructions")
MODEL = "gpt-4-turbo"
MAX_TOKENS = 1800
KEY_LINE_PROMPT = """
–¢—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –∫–æ–º–ø–∞–Ω–∏–∏, –Ω–∞–ø–∏—Å–∞–Ω–Ω—É—é –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

–≠—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ–±—ã—á–Ω–æ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å –≤–≤–æ–¥–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—Ä–æ–∫–∏, —Ä–æ–ª–∏, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏), –∞ —Ç–æ–ª—å–∫–æ –ø–æ—Ç–æ–º –ø–µ—Ä–µ—Ö–æ–¥—è—Ç –∫ –ø–æ—à–∞–≥–æ–≤—ã–º –¥–µ–π—Å—Ç–≤–∏—è–º.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É, —Å –∫–æ—Ç–æ—Ä–æ–π –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –∏–º–µ–Ω–Ω–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏.

–û—Ç–≤–µ—Ç—å –¢–û–ß–ù–û —ç—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π - –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤, –ø–æ—è—Å–Ω–µ–Ω–∏–π –∏–ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤. –ï—Å–ª–∏ —Ç–æ—á–Ω–æ –Ω–µ —É–≤–µ—Ä–µ–Ω - –≤—ã–±–µ—Ä–∏ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â—É—é —Å—Ç—Ä–æ–∫—É.
"""

MAIN_PROMPT = """
–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
–í–ê–ñ–ù–û: –ù–µ –æ–±–æ—Ä–∞—á–∏–≤–∞–π –≤—ã–≤–æ–¥ –≤ Markdown. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Ç—Ä–æ–π–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏, –∫–∞–≤—ã—á–∫–∏ –≤–æ–∫—Ä—É–≥ JSON –∏–ª–∏ –ª—é–±—ã–µ —Å–∏–º–≤–æ–ª—ã –¥–æ –∏–ª–∏ –ø–æ—Å–ª–µ JSON. –í—ã–≤–æ–¥–∏ —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–π JSON.

–¢—ã - —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –¢–µ–±–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã:
- intro - —Ç–µ–∫—Å—Ç –≤–≤–æ–¥–Ω–æ–π —á–∞—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –¥–æ –Ω–∞—á–∞–ª–∞ —à–∞–≥–æ–≤
- steps - —Å–ø–∏—Å–æ–∫ —à–∞–≥–æ–≤, –∫–∞–∂–¥—ã–π –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –º–æ–∂–µ—Ç –≤–∫–ª—é—á–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

–°—Ñ–æ—Ä–º–∏—Ä—É–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON —Å–ª–µ–¥—É—é—â–µ–≥–æ –≤–∏–¥–∞:
{
  "task_id": "string",
  "title": "string",
  "intro": "string",
  "steps": [
    {
      "step_num": 1,
      "text": "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è...",
      "images": ["image_0.png", "image_1.png"],
      "keywords": ["–ø–æ–ª–µ", "—Ñ–∏–ª—å—Ç—Ä", "–∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç"]
    }
  ]
}

title –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º –∏ –æ—Ç—Ä–∞–∂–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é —Ü–µ–ª—å –∑–∞–¥–∞—á–∏ (–≤–≤–æ–¥–Ω–∞—è —Ç–µ–±–µ –ø–æ–º–æ–∂–µ—Ç).
intro –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤—Å—Ç–∞–≤–ª–µ–Ω –∫–∞–∫ –µ–¥–∏–Ω–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø–æ–ª–µ (–≤—Å—è –≤–≤–æ–¥–Ω–∞—è —á–∞—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞).
steps - —Ç–æ–ª—å–∫–æ –¥–µ–π—Å—Ç–≤–∏—è. –ù–µ –¥—É–±–ª–∏—Ä—É–π –∞–±–∑–∞—Ü—ã –∏–∑ intro —Ç—É–¥–∞.
"""

openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)

def extract_text(docx_path):
    doc = Document(docx_path)
    return "\n".join(p.text.strip() for p in doc.paragraphs if p.text.strip())

def detect_entry_line(docx_path):
    text = extract_text(docx_path)
    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "system", "content": KEY_LINE_PROMPT},
                  {"role": "user", "content": text}]
    )
    return response.choices[0].message.content.strip()

def parse_docx_to_elements(docx_path, img_dir):
    elements = []
    os.makedirs(img_dir, exist_ok=True)
    with ZipFile(docx_path) as z:
        doc_xml = z.read("word/document.xml")
        rels_xml = z.read("word/_rels/document.xml.rels")
        media = {n: z.read(n) for n in z.namelist() if n.startswith("word/media/")}

    rels = etree.fromstring(rels_xml)
    rel_map = {
        r.attrib["Id"]: r.attrib["Target"]
        for r in rels if r.attrib["Target"].startswith("media/")
    }

    tree = etree.fromstring(doc_xml)
    nsmap = tree.nsmap
    body = tree.find(".//w:body", namespaces=nsmap)

    img_count = 0
    for node in body:
        if etree.QName(node).localname != "p":
            continue
        text = "".join(node.itertext()).strip()
        if text:
            elements.append({ "type": "normal", "text": text })

        for blip in node.findall(".//a:blip", namespaces=nsmap):
            rId = blip.attrib.get("{http://schemas.openxmlformats.org/officeDocument/2006/relationships}embed")
            filename = rel_map.get(rId)
            if filename:
                img_bytes = media.get("word/" + filename)
                img_name = f"image_{img_count}.png"
                with open(img_dir / img_name, "wb") as f:
                    f.write(img_bytes)
                elements.append({ "type": "image", "placeholder": img_name })
                img_count += 1

    return elements

def merge_images_to_previous(elements):
    steps = []
    for el in elements:
        if el["type"] == "normal" and el["text"].strip():
            steps.append({ "type": "step", "text": el["text"].strip(), "images": [] })
        elif el["type"] == "image" and steps:
            steps[-1]["images"].append(el["placeholder"])
    return steps

def process_docx(docx_path):
    task_id = docx_path.stem
    task_dir = OUT_DIR / task_id
    img_dir = task_dir / "img"
    os.makedirs(task_dir, exist_ok=True)

    print(f"üìÑ Processing {task_id}...")

    elements = parse_docx_to_elements(docx_path, img_dir)
    with open(task_dir / "docx_parsed.json", "w", encoding="utf-8") as f:
        json.dump(elements, f, ensure_ascii=False, indent=2)

    entry_line = detect_entry_line(docx_path)
    split_index = next((i for i, el in enumerate(elements) if el.get("text") == entry_line), 0)
    intro = "\n".join(el["text"] for el in elements[:split_index] if el["type"] == "normal")

    # Trim intro if too long
    if len(intro) > 1000:
        intro = intro[:1000] + "..."

    steps = merge_images_to_previous(elements[split_index:])
    payload = { "intro": intro, "steps": steps }

    print("ü§ñ Calling GPT for structure...")
    response = openai_client.chat.completions.create(
        model=MODEL,
        max_tokens=MAX_TOKENS,
        temperature=0.2,
        messages=[
            {"role": "system", "content": MAIN_PROMPT},
            {"role": "user", "content": json.dumps(payload, ensure_ascii=False)}
        ]
    )

    text = response.choices[0].message.content.strip()

    # Soft validation and trim fallback
    if text.startswith("```json"):
        text = text.removeprefix("```json").removesuffix("```").strip()
    if not text.endswith("}"):
        print("‚ö†Ô∏è GPT response was cut off. Attempting soft trim...")
        last_valid = text.rfind("},")
        if last_valid != -1:
            text = text[:last_valid+1] + "\n  ]\n}"

    try:
        structured = json.loads(text)
        with open(task_dir / "structured_output.json", "w", encoding="utf-8") as f:
            json.dump(structured, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ Saved to {task_dir}/structured_output.json")
    except json.JSONDecodeError:
        print(f"‚ùå GPT returned invalid JSON for {task_id}")
        print(text)

def run():
    for docx_file in RAW_DIR.glob("*.docx"):
        process_docx(docx_file)

if __name__ == "__main__":
    run()
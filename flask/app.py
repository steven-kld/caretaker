from flask import Flask, request, jsonify, Response
from flask_cors import CORS
import os, openai, base64
from io import BytesIO

openai.api_key = os.getenv("OPENAI_API_KEY")

app = Flask(__name__)
CORS(app)

UPLOAD_FOLDER = './temp'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

@app.route("/api/session/init")
def init_session():
    return jsonify({"ok": os.getenv("OPENAI_API_KEY")})

@app.route('/api/process', methods=['POST'])
def process():
    try:
        audio = request.files.get('audio')
        if not audio or not hasattr(audio.stream, 'read'):
            return jsonify({'error': 'No valid audio file'}), 400

        # üß† Read the binary content once
        raw_bytes = audio.read()
        if not raw_bytes:
            return jsonify({'error': 'Empty audio stream'}), 400

        # üíæ Wrap in BytesIO for OpenAI
        buffer = BytesIO(raw_bytes)
        buffer.name = audio.filename  # required by OpenAI SDK

        # üîë Create client per request
        client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

        # üß† Transcribe with Whisper
        response = client.audio.transcriptions.create(
            model="whisper-1",
            file=buffer,
            response_format="verbose_json"
        )
        if not response.text:
            return jsonify({'error': 'Missing text'}), 400

        # NEW THINGS HERE
        image_files = request.files.getlist("images")
        vision_parts = []
        for img in image_files[:5]:  # safety cap: max 1 image
            b64 = base64.b64encode(img.read()).decode("utf-8")
            vision_parts.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{b64}"}
            })
        
        mp3_data = generate_response(response.text, vision_parts)
        if not mp3_data:
            return jsonify({'error': 'TTS failed'}), 500

        return Response(mp3_data, mimetype="audio/mpeg")

    except Exception as e:
        print("‚ùå Whisper failed:", e)
        return jsonify({'error': str(e)}), 500

def generate_speech(client, text, voice="nova"):
    try:
        response = client.audio.speech.create(
            model="tts-1",
            voice=voice,
            input=text
        )
        return response.content  # MP3 binary

    except Exception as e:
        print("‚ùå TTS failed:", e)
        return None
    
def generate_response(text, vision_parts, lang: str = "ru") -> str:
    client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    # üì¶ Abstract, context-aware prompt
    prompt = [
        {"type": "text", "text": "–ù–∞ –æ—Å–Ω–æ–≤–µ —ç–∫—Ä–∞–Ω–∞ –∏ —Ä–µ—á–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ–ø—Ä–µ–¥–µ–ª–∏ –µ–≥–æ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –∏ –ø–æ–¥—Å–∫–∞–∂–∏, —á—Ç–æ –¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ."}
    ] + vision_parts + [
        {"type": "text", "text": f"–ì–æ–ª–æ—Å–æ–≤–æ–π –≤–≤–æ–¥: {text}"}
    ]

    try:
        chat = client.chat.completions.create(
            model="gpt-4o" if vision_parts else "gpt-4",
            messages=[
                {"role": "system", "content": "–¢—ã –∫—Ä–∞—Ç–∫–∏–π, —É–≤–µ—Ä–µ–Ω–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –ò—Å–ø–æ–ª—å–∑—É–π —ç–∫—Ä–∞–Ω –∏ –≥–æ–ª–æ—Å, —á—Ç–æ–±—ã —É–≥–∞–¥–∞—Ç—å, —á—Ç–æ —Ö–æ—á–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å, –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥. –ù–µ –æ–±—ä—è—Å–Ω—è–π, –ø—Ä–æ—Å—Ç–æ –ø–æ–¥—Å–∫–∞–∂–∏."},
                {"role": "user", "content": prompt if vision_parts else text}
            ]
        )

        reply = chat.choices[0].message.content.strip()
        if not reply or len(reply) < 10:
            reply = "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ —Å–º–æ–≥ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥."

        return generate_speech(client, reply)

    except Exception as e:
        print("‚ùå GPT error:", e)
        return None

    
if __name__ == '__main__':
    app.run(host='0.0.0.0', port=9091, debug=True)